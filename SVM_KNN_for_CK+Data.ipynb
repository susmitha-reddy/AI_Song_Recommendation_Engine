{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "emotions = [\"happy\",\"sadness\",\"surprise\",\"fear\",\"disgust\",\"anger\",\"contempt\"]\n",
    "\n",
    "#Fetching the landmarks dat which contains the pre defined model\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "model = dlib.shape_predictor(\"C:\\\\Users\\\\Neha\\\\Desktop\\\\AIProject\\\\shape_predictor_68_face_landmarks.dat\")\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "# Set the classifier as a support vector machines with linear kernel\n",
    "n_estimators = 10\n",
    "clf_new = OneVsRestClassifier(BaggingClassifier(SVC(kernel='linear', probability=True), max_samples=1.0 / n_estimators, n_estimators=n_estimators))\n",
    "clf = SVC(C=0.01, kernel='linear', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    # For all detected face instances individually\n",
    "    for k, d in enumerate(detections):\n",
    "        # Get facial landmarks with prediction model\n",
    "        shape = model(image, d)\n",
    "        xpoint = []\n",
    "        ypoint = []\n",
    "        for i in range(0, 68):\n",
    "            xpoint.append(float(shape.part(i).x))\n",
    "            ypoint.append(float(shape.part(i).y))\n",
    "\n",
    "        # Center points of both axis\n",
    "        xcenter = np.mean(xpoint)\n",
    "        ycenter = np.mean(ypoint)\n",
    "        # Calculate distance between particular points and center point\n",
    "        xdistcent = [(x-xcenter) for x in xpoint]\n",
    "        ydistcent = [(y-ycenter) for y in ypoint]\n",
    "\n",
    "        # Prevent divided by 0 value\n",
    "        if xpoint[11] == xpoint[14]:\n",
    "            angle_nose = 0\n",
    "        else:\n",
    "            # Point 14 is the tip of the nose, point 11 is the top of the nose brigde\n",
    "            angle_nose = int(math.atan((ypoint[11]-ypoint[14])/(xpoint[11]-xpoint[14]))*180/math.pi)\n",
    "\n",
    "        # Get offset by finding how the nose brigde should be rotated to become perpendicular to the horizontal plane\n",
    "        if angle_nose < 0:\n",
    "            angle_nose += 90\n",
    "        else:\n",
    "            angle_nose -= 90\n",
    "\n",
    "        landmarks = []\n",
    "        for cx, cy, x, y in zip(xdistcent, ydistcent, xpoint, ypoint):\n",
    "            # Add the coordinates relative to the centre of gravity\n",
    "            landmarks.append(cx)\n",
    "            landmarks.append(cy)\n",
    "\n",
    "            # Get the euclidean distance between each point and the centre point (the vector length)\n",
    "            meanar = np.asarray((ycenter,xcenter))\n",
    "            centpar = np.asarray((y,x))\n",
    "            dist = np.linalg.norm(centpar-meanar)\n",
    "\n",
    "            # Get the angle the vector describes relative to the image, corrected for the offset that the nosebrigde\n",
    "            # has when the face is not perfectly horizontal\n",
    "            if x == xcenter:\n",
    "                angle_relative = 0\n",
    "            else:\n",
    "                angle_relative = (math.atan(float(y-ycenter)/(x-xcenter))*180/math.pi) - angle_nose\n",
    "            landmarks.append(dist)\n",
    "            landmarks.append(angle_relative)\n",
    "\n",
    "    if len(detections) < 1:\n",
    "        # In case no case selected, print \"error\" values\n",
    "        landmarks = \"error\"\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def get_files(emotion):\n",
    "    images = glob.glob(\"C:\\\\Users\\\\Neha\\\\Desktop\\\\AIProject\\\\CK_DATASET\\\\CK_DATASET\\\\CK+48\\\\%s\\\\*\" %emotion)\n",
    "    random.shuffle(images)\n",
    "    #get 80% of image files to be trained\n",
    "    training_set = images[:int(len(images)*0.8)]\n",
    "    #get 20% of image files to be tested\n",
    "    testing_set = images[-int(len(images)*0.2):]   \n",
    "    return training_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_label = []\n",
    "    testing_data = []\n",
    "    testing_label = []\n",
    "    for emotion in emotions:\n",
    "        training_set, testing_set = get_files(emotion)\n",
    "        print(\"length of train data set:\", len(training_set))\n",
    "        print('length of test data set:',testing_set)\n",
    "        #add data to training and testing dataset, and generate labels 0-4\n",
    "        for item in training_set:\n",
    "            #read image\n",
    "            img = cv2.imread(item)\n",
    "            #convert to grayscale\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_img = clahe.apply(gray_img)\n",
    "            landmarks_vec = get_landmarks(clahe_img)\n",
    "\n",
    "            if landmarks_vec == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                training_data.append(landmarks_vec)\n",
    "                training_label.append(emotions.index(emotion))\n",
    "\n",
    "        for item in testing_set:\n",
    "            img = cv2.imread(item)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            clahe_img = clahe.apply(gray_img)\n",
    "            landmarks_vec = get_landmarks(clahe_img)\n",
    "            if landmarks_vec == \"error\":\n",
    "                pass\n",
    "            else:\n",
    "                testing_data.append(landmarks_vec)\n",
    "                testing_label.append(emotions.index(emotion))\n",
    "\n",
    "    return training_data, training_label, testing_data, testing_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 165\n",
      "length: 67\n",
      "length: 199\n",
      "length: 60\n",
      "length: 141\n",
      "length: 108\n",
      "length: 43\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] ................ C=0.1, kernel=linear, score=0.815, total=   0.0s\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] ................ C=0.1, kernel=linear, score=0.898, total=   0.0s\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] ................ C=0.1, kernel=linear, score=0.924, total=   0.1s\n",
      "[CV] C=0.1, kernel=linear ............................................\n",
      "[CV] ................ C=0.1, kernel=linear, score=0.885, total=   0.1s\n",
      "[CV] C=0.1, kernel=linear ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ C=0.1, kernel=linear, score=0.878, total=   0.1s\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] .................. C=1, kernel=linear, score=0.815, total=   0.0s\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] .................. C=1, kernel=linear, score=0.892, total=   0.0s\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] .................. C=1, kernel=linear, score=0.911, total=   0.0s\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] .................. C=1, kernel=linear, score=0.891, total=   0.1s\n",
      "[CV] C=1, kernel=linear ..............................................\n",
      "[CV] .................. C=1, kernel=linear, score=0.878, total=   0.1s\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] ................. C=10, kernel=linear, score=0.815, total=   0.0s\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] ................. C=10, kernel=linear, score=0.892, total=   0.1s\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] ................. C=10, kernel=linear, score=0.911, total=   0.1s\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] ................. C=10, kernel=linear, score=0.891, total=   0.1s\n",
      "[CV] C=10, kernel=linear .............................................\n",
      "[CV] ................. C=10, kernel=linear, score=0.878, total=   0.0s\n",
      "[CV] C=0.01, kernel=linear ...........................................\n",
      "[CV] ............... C=0.01, kernel=linear, score=0.854, total=   0.0s\n",
      "[CV] C=0.01, kernel=linear ...........................................\n",
      "[CV] ............... C=0.01, kernel=linear, score=0.892, total=   0.1s\n",
      "[CV] C=0.01, kernel=linear ...........................................\n",
      "[CV] ............... C=0.01, kernel=linear, score=0.904, total=   0.1s\n",
      "[CV] C=0.01, kernel=linear ...........................................\n",
      "[CV] ............... C=0.01, kernel=linear, score=0.878, total=   0.1s\n",
      "[CV] C=0.01, kernel=linear ...........................................\n",
      "[CV] ............... C=0.01, kernel=linear, score=0.885, total=   0.1s\n",
      "[CV] C=0.001, kernel=linear ..........................................\n",
      "[CV] .............. C=0.001, kernel=linear, score=0.752, total=   0.1s\n",
      "[CV] C=0.001, kernel=linear ..........................................\n",
      "[CV] .............. C=0.001, kernel=linear, score=0.796, total=   0.1s\n",
      "[CV] C=0.001, kernel=linear ..........................................\n",
      "[CV] .............. C=0.001, kernel=linear, score=0.783, total=   0.1s\n",
      "[CV] C=0.001, kernel=linear ..........................................\n",
      "[CV] .............. C=0.001, kernel=linear, score=0.795, total=   0.1s\n",
      "[CV] C=0.001, kernel=linear ..........................................\n",
      "[CV] .............. C=0.001, kernel=linear, score=0.795, total=   0.1s\n",
      "[CV] C=0.0001, kernel=linear .........................................\n",
      "[CV] ............. C=0.0001, kernel=linear, score=0.554, total=   0.1s\n",
      "[CV] C=0.0001, kernel=linear .........................................\n",
      "[CV] ............. C=0.0001, kernel=linear, score=0.529, total=   0.1s\n",
      "[CV] C=0.0001, kernel=linear .........................................\n",
      "[CV] ............. C=0.0001, kernel=linear, score=0.567, total=   0.1s\n",
      "[CV] C=0.0001, kernel=linear .........................................\n",
      "[CV] ............. C=0.0001, kernel=linear, score=0.571, total=   0.1s\n",
      "[CV] C=0.0001, kernel=linear .........................................\n",
      "[CV] ............. C=0.0001, kernel=linear, score=0.532, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['linear']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10,0.01, 0.001, 0.0001],\n",
    "              'kernel': ['linear']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "X_train, y_train, X_test, y_test = make_sets()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'kernel': 'linear'}\n",
      "SVC(C=0.01, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        41\n",
      "           1       0.67      0.75      0.71        16\n",
      "           2       1.00      0.98      0.99        49\n",
      "           3       1.00      0.87      0.93        15\n",
      "           4       0.94      0.97      0.96        35\n",
      "           5       0.80      0.89      0.84        27\n",
      "           6       0.62      0.50      0.56        10\n",
      "\n",
      "    accuracy                           0.91       193\n",
      "   macro avg       0.86      0.85      0.85       193\n",
      "weighted avg       0.91      0.91      0.91       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictions = grid.predict(X_test_scaled)\n",
    "# print classification report\n",
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 165\n",
      "length: 67\n",
      "length: 199\n",
      "length: 60\n",
      "length: 141\n",
      "length: 108\n",
      "length: 43\n",
      "783\n",
      "783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train_knn, y_train_knn, X_test_knn, y_test_knn = make_sets()\n",
    "print(len(X_train_knn))\n",
    "print(len(y_train_knn))\n",
    "X_train_knn_arr = np.array(X_train_knn)\n",
    "Y_train_knn_arr = np.array(y_train_knn)\n",
    "X_test_knn_arr = np.array(X_test_knn)\n",
    "Y_test_knn_arr = np.array(y_test_knn)\n",
    "X_train_knn_arr_scaled = scaler.fit_transform(X_train_knn_arr)\n",
    "X_test_knn_arr_scaled = scaler.transform(X_test_knn_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(783, 272)\n",
      "(783,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_knn_arr_scaled.shape)\n",
    "print(Y_train_knn_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': np.arange(1, 30) }\n",
    "knn_gridcv = GridSearchCV(clf_knn, param_grid ,cv=10, refit = True,n_jobs= -1)\n",
    "knn_gridcv.fit(X_train_knn_arr_scaled,Y_train_knn_arr)\n",
    "knn_gridcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92594937, 0.84543979, 0.82370983, 0.80197988, 0.77387212,\n",
       "       0.77510549, 0.75725414, 0.75082765, 0.74952937, 0.7507952 ,\n",
       "       0.74055501, 0.7316618 , 0.73162934, 0.73161311, 0.72010711,\n",
       "       0.71752678, 0.71496267, 0.71366439, 0.71366439, 0.71239857,\n",
       "       0.70217462, 0.70730282, 0.70473872, 0.70342421, 0.7009088 ,\n",
       "       0.6958455 , 0.69712756, 0.69967543, 0.70350536])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = knn_gridcv.cv_results_['mean_test_score']\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91        41\n",
      "           1       0.93      0.88      0.90        16\n",
      "           2       1.00      0.92      0.96        49\n",
      "           3       0.94      1.00      0.97        15\n",
      "           4       0.85      0.80      0.82        35\n",
      "           5       0.71      0.93      0.81        27\n",
      "           6       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.90       193\n",
      "   macro avg       0.90      0.91      0.90       193\n",
      "weighted avg       0.91      0.90      0.90       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_predictin_random= knn_gridcv.predict(X_test_knn_arr_scaled)\n",
    "print(classification_report(Y_test_knn_arr,grid_predictin_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
